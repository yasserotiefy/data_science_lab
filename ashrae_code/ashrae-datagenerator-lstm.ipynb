{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to the ASHRAE - Great Energy Predictor Competition\nThis notebook is a starter code for all beginners and easy to understand. The train and test data are very large so we will work with a data generator based on the template to generate the data on the fly <br>\nhttps://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n\nAdditionally we follow an efficient workflow. <br>\nWe also use categorical feature encoding techniques, compare <br>\nhttps://www.kaggle.com/drcapa/categorical-feature-encoding-challenge-xgb\n\nFor the first step we will take a simple neural network based on the keras library. After that we will use a RNN.<br>\nCurrent status of the kernel: The workflow is complete.<br>\nNext steps: \n* Improve the LSTM.\n* Expand the feature engineering based on the kernel: https://www.kaggle.com/drcapa/ashrae-feature-engineering","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.special\nimport matplotlib.pyplot as plt\nimport os\nimport random","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-01-04T09:51:15.630286Z","iopub.execute_input":"2022-01-04T09:51:15.630649Z","iopub.status.idle":"2022-01-04T09:51:15.945131Z","shell.execute_reply.started":"2022-01-04T09:51:15.630594Z","shell.execute_reply":"2022-01-04T09:51:15.943992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\nfrom keras.optimizers import RMSprop,Adam\nimport keras.backend as K","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:15.947517Z","iopub.execute_input":"2022-01-04T09:51:15.947883Z","iopub.status.idle":"2022-01-04T09:51:18.053273Z","shell.execute_reply.started":"2022-01-04T09:51:15.947829Z","shell.execute_reply":"2022-01-04T09:51:18.052477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:18.054798Z","iopub.execute_input":"2022-01-04T09:51:18.055339Z","iopub.status.idle":"2022-01-04T09:51:18.958063Z","shell.execute_reply.started":"2022-01-04T09:51:18.055266Z","shell.execute_reply":"2022-01-04T09:51:18.95731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:18.959222Z","iopub.execute_input":"2022-01-04T09:51:18.95962Z","iopub.status.idle":"2022-01-04T09:51:18.96326Z","shell.execute_reply.started":"2022-01-04T09:51:18.959581Z","shell.execute_reply":"2022-01-04T09:51:18.962688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"path_in = '../input/ashrae-energy-prediction/'\nprint(os.listdir(path_in))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:18.967299Z","iopub.execute_input":"2022-01-04T09:51:18.967959Z","iopub.status.idle":"2022-01-04T09:51:18.976292Z","shell.execute_reply.started":"2022-01-04T09:51:18.967822Z","shell.execute_reply":"2022-01-04T09:51:18.975304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(path_in+'train.csv', parse_dates=['timestamp'])\ntrain_weather = pd.read_csv(path_in+'weather_train.csv', parse_dates=['timestamp'])\nbuilding_data = pd.read_csv(path_in+'building_metadata.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:18.977857Z","iopub.execute_input":"2022-01-04T09:51:18.978422Z","iopub.status.idle":"2022-01-04T09:51:36.092569Z","shell.execute_reply.started":"2022-01-04T09:51:18.978369Z","shell.execute_reply":"2022-01-04T09:51:36.091396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Help function","metadata":{}},{"cell_type":"code","source":"def plot_bar(data, name):\n    fig = plt.figure(figsize=(16, 9))\n    ax = fig.add_subplot(111)\n    data_label = data[name].value_counts()\n    dict_train = dict(zip(data_label.keys(), ((data_label.sort_index())).tolist()))\n    names = list(dict_train.keys())\n    values = list(dict_train.values())\n    plt.bar(names, values)\n    ax.set_xticklabels(names, rotation=45)\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:36.094416Z","iopub.execute_input":"2022-01-04T09:51:36.094806Z","iopub.status.idle":"2022-01-04T09:51:36.10264Z","shell.execute_reply.started":"2022-01-04T09:51:36.094733Z","shell.execute_reply":"2022-01-04T09:51:36.101238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handle missing values of building and weather data\nThe missing data are numerical values. So for the first step we can use a simple imputer of the sklearn library.","metadata":{}},{"cell_type":"code","source":"cols_with_missing_train_weather = [col for col in train_weather.columns if train_weather[col].isnull().any()]\ncols_with_missing_building = [col for col in building_data.columns if building_data[col].isnull().any()]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:36.104153Z","iopub.execute_input":"2022-01-04T09:51:36.104424Z","iopub.status.idle":"2022-01-04T09:51:36.123782Z","shell.execute_reply.started":"2022-01-04T09:51:36.104379Z","shell.execute_reply":"2022-01-04T09:51:36.121818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cols_with_missing_train_weather)\nprint(cols_with_missing_building)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:36.126243Z","iopub.execute_input":"2022-01-04T09:51:36.126902Z","iopub.status.idle":"2022-01-04T09:51:36.136009Z","shell.execute_reply.started":"2022-01-04T09:51:36.126786Z","shell.execute_reply":"2022-01-04T09:51:36.134868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp_most = SimpleImputer(strategy='most_frequent')\ntrain_weather[cols_with_missing_train_weather] = imp_most.fit_transform(train_weather[cols_with_missing_train_weather])\nbuilding_data[cols_with_missing_building] = imp_most.fit_transform(building_data[cols_with_missing_building])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:36.137649Z","iopub.execute_input":"2022-01-04T09:51:36.137986Z","iopub.status.idle":"2022-01-04T09:51:36.351016Z","shell.execute_reply.started":"2022-01-04T09:51:36.137928Z","shell.execute_reply":"2022-01-04T09:51:36.350294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scale objective label","metadata":{}},{"cell_type":"code","source":"train_data['meter_reading'] = np.log1p(train_data['meter_reading'])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:36.351993Z","iopub.execute_input":"2022-01-04T09:51:36.35238Z","iopub.status.idle":"2022-01-04T09:51:36.553426Z","shell.execute_reply.started":"2022-01-04T09:51:36.35234Z","shell.execute_reply":"2022-01-04T09:51:36.552079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create new features\n## Train data\nBased on the timestamp we create new features which are cyclic.","metadata":{}},{"cell_type":"code","source":"train_data['month'] = train_data['timestamp'].dt.month\ntrain_data['day'] = train_data['timestamp'].dt.weekday\ntrain_data['year'] = train_data['timestamp'].dt.year\ntrain_data['hour'] = train_data['timestamp'].dt.hour","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:36.554596Z","iopub.execute_input":"2022-01-04T09:51:36.554913Z","iopub.status.idle":"2022-01-04T09:51:44.917296Z","shell.execute_reply.started":"2022-01-04T09:51:36.554869Z","shell.execute_reply":"2022-01-04T09:51:44.916258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Additionally we create the feature weekend: 5 = saturday and 6 = sunday.","metadata":{}},{"cell_type":"code","source":"train_data['weekend'] = np.where((train_data['day'] == 5) | (train_data['day'] == 6), 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:44.918696Z","iopub.execute_input":"2022-01-04T09:51:44.919152Z","iopub.status.idle":"2022-01-04T09:51:45.510249Z","shell.execute_reply.started":"2022-01-04T09:51:44.918961Z","shell.execute_reply":"2022-01-04T09:51:45.509209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weather data\nThe feature wind_direction is cyclic.","metadata":{}},{"cell_type":"code","source":"train_weather['wind_direction'+'_sin'] = np.sin((2*np.pi*train_weather['wind_direction'])/360)\ntrain_weather['wind_direction'+'_cos'] = np.cos((2*np.pi*train_weather['wind_direction'])/360)\ntrain_weather = train_weather.drop(['wind_direction'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:45.511386Z","iopub.execute_input":"2022-01-04T09:51:45.511616Z","iopub.status.idle":"2022-01-04T09:51:45.570992Z","shell.execute_reply.started":"2022-01-04T09:51:45.511578Z","shell.execute_reply":"2022-01-04T09:51:45.570231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\nThere is a greate encoding competition: https://www.kaggle.com/drcapa/categorical-feature-encoding-challenge-xgb\n## Train data\n### Feature meter\nThere are 4 types of meters: <br>\n0 = electricity, 1 = chilledwater, 2 = steam, 3 = hotwater <br>\nWe use the one hot encoding for this 4 feature.","metadata":{}},{"cell_type":"code","source":"train_data = pd.get_dummies(train_data, columns=['meter'])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:45.571999Z","iopub.execute_input":"2022-01-04T09:51:45.572346Z","iopub.status.idle":"2022-01-04T09:51:49.755293Z","shell.execute_reply.started":"2022-01-04T09:51:45.572308Z","shell.execute_reply":"2022-01-04T09:51:49.75441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features month, day and hour\nWe created the features month, day and hour which are cyclic.","metadata":{}},{"cell_type":"code","source":"features_cyc = {'month' : 12, 'day' : 7, 'hour' : 24}\nfor feature in features_cyc.keys():\n    train_data[feature+'_sin'] = np.sin((2*np.pi*train_data[feature])/features_cyc[feature])\n    train_data[feature+'_cos'] = np.cos((2*np.pi*train_data[feature])/features_cyc[feature])\ntrain_data = train_data.drop(features_cyc.keys(), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:49.757162Z","iopub.execute_input":"2022-01-04T09:51:49.757718Z","iopub.status.idle":"2022-01-04T09:51:55.485464Z","shell.execute_reply.started":"2022-01-04T09:51:49.757481Z","shell.execute_reply":"2022-01-04T09:51:55.484808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building data\nThe feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping.","metadata":{}},{"cell_type":"code","source":"plot_bar(building_data, 'primary_use')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.486679Z","iopub.execute_input":"2022-01-04T09:51:55.486958Z","iopub.status.idle":"2022-01-04T09:51:55.844823Z","shell.execute_reply.started":"2022-01-04T09:51:55.486904Z","shell.execute_reply":"2022-01-04T09:51:55.843882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_use = dict(zip(building_data['primary_use'].value_counts().sort_index().keys(),\n                     range(1, len(building_data['primary_use'].value_counts())+1)))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.846027Z","iopub.execute_input":"2022-01-04T09:51:55.846379Z","iopub.status.idle":"2022-01-04T09:51:55.857772Z","shell.execute_reply.started":"2022-01-04T09:51:55.846331Z","shell.execute_reply":"2022-01-04T09:51:55.856472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"building_data['primary_use'] = building_data['primary_use'].replace(map_use)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.859579Z","iopub.execute_input":"2022-01-04T09:51:55.860151Z","iopub.status.idle":"2022-01-04T09:51:55.878077Z","shell.execute_reply.started":"2022-01-04T09:51:55.859911Z","shell.execute_reply":"2022-01-04T09:51:55.877284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#building_data = pd.get_dummies(building_data, columns=['primary_use'])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.879621Z","iopub.execute_input":"2022-01-04T09:51:55.880175Z","iopub.status.idle":"2022-01-04T09:51:55.886327Z","shell.execute_reply.started":"2022-01-04T09:51:55.880126Z","shell.execute_reply":"2022-01-04T09:51:55.885299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scale building and weather data\n## Weather data","metadata":{}},{"cell_type":"code","source":"weather_scale = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'sea_level_pressure', 'wind_speed']","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.889406Z","iopub.execute_input":"2022-01-04T09:51:55.889963Z","iopub.status.idle":"2022-01-04T09:51:55.89917Z","shell.execute_reply.started":"2022-01-04T09:51:55.889902Z","shell.execute_reply":"2022-01-04T09:51:55.898171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = train_weather[weather_scale].mean(axis=0)\ntrain_weather[weather_scale] = train_weather[weather_scale].astype('float32')\ntrain_weather[weather_scale] -= train_weather[weather_scale].mean(axis=0)\nstd = train_weather[weather_scale].std(axis=0)\ntrain_weather[weather_scale] /= train_weather[weather_scale].std(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.901058Z","iopub.execute_input":"2022-01-04T09:51:55.901692Z","iopub.status.idle":"2022-01-04T09:51:55.97415Z","shell.execute_reply.started":"2022-01-04T09:51:55.901617Z","shell.execute_reply":"2022-01-04T09:51:55.973124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building data","metadata":{}},{"cell_type":"code","source":"building_scale = ['square_feet', 'year_built', 'floor_count']","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.975852Z","iopub.execute_input":"2022-01-04T09:51:55.976447Z","iopub.status.idle":"2022-01-04T09:51:55.980984Z","shell.execute_reply.started":"2022-01-04T09:51:55.976386Z","shell.execute_reply":"2022-01-04T09:51:55.98015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = building_data[building_scale].mean(axis=0)\nbuilding_data[building_scale] = building_data[building_scale].astype('float32')\nbuilding_data[building_scale] -= building_data[building_scale].mean(axis=0)\nstd = building_data[building_scale].std(axis=0)\nbuilding_data[building_scale] /= building_data[building_scale].std(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:55.98271Z","iopub.execute_input":"2022-01-04T09:51:55.983332Z","iopub.status.idle":"2022-01-04T09:51:56.016499Z","shell.execute_reply.started":"2022-01-04T09:51:55.983246Z","shell.execute_reply":"2022-01-04T09:51:56.015777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge data","metadata":{}},{"cell_type":"code","source":"train_data = pd.merge(train_data, building_data, on='building_id', right_index=True)\ntrain_data = train_data.sort_values(['timestamp'])\ntrain_data = pd.merge_asof(train_data, train_weather, on='timestamp', by='site_id', right_index=True)\ndel train_weather","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:51:56.017498Z","iopub.execute_input":"2022-01-04T09:51:56.017912Z","iopub.status.idle":"2022-01-04T09:52:28.323441Z","shell.execute_reply.started":"2022-01-04T09:51:56.017853Z","shell.execute_reply":"2022-01-04T09:52:28.32235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.to_csv(\"../working/ashrae_merged_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:04:45.579042Z","iopub.execute_input":"2022-01-04T10:04:45.579433Z","iopub.status.idle":"2022-01-04T10:17:04.622145Z","shell.execute_reply.started":"2022-01-04T10:04:45.579378Z","shell.execute_reply":"2022-01-04T10:17:04.618173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the data generator","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    \"\"\" A data generator based on the template\n        https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n        \"\"\"\n    \n    def __init__(self, data, list_IDs, features, batch_size, shuffle=False):\n        self.data = data.loc[list_IDs].copy()\n        self.list_IDs = list_IDs\n        self.features = features\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    \n    def __len__(self):\n        return int(np.floor(len(self.list_IDs)/self.batch_size))\n    \n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n    \n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n        \n    def __data_generation(self, list_IDs_temp):        \n        X = np.empty((len(list_IDs_temp), len(self.features)), dtype=float)\n        y = np.empty((len(list_IDs_temp), 1), dtype=float)\n        X = self.data.loc[list_IDs_temp, self.features].values\n        \n        if 'meter_reading' in self.data.columns:\n            y = self.data.loc[list_IDs_temp, 'meter_reading'].values\n        # reshape\n        X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n        return X, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split the random input data into train and val\nSince it's a timeseries problem, we split the train and validation data by timestamp and not with a random split.","metadata":{}},{"cell_type":"code","source":"train_size = int(len(train_data.index)*0.75)\nval_size = len(train_data.index) - train_size\ntrain_list, val_list = train_data.index[0:train_size], train_data.index[train_size:train_size+val_size]\nprint(train_size, val_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the features","metadata":{}},{"cell_type":"code","source":"no_features = ['building_id', 'timestamp', 'meter_reading', 'year']\nfeatures = train_data.columns.difference(no_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define train and validation data via Data Generator","metadata":{}},{"cell_type":"code","source":"batch_size = 1024\ntrain_generator = DataGenerator(train_data, train_list, features, batch_size)\nval_generator = DataGenerator(train_data, val_list, features, batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Recurrent Neural Network\nWe use a simple recurrent neural network for train and prediction. Later we will improve.","metadata":{}},{"cell_type":"code","source":"input_dim = len(features)\nprint(input_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n#model.add(Embedding(input_length=input_dim))\nmodel.add(LSTM(units=8, activation = 'relu', input_shape=(1, input_dim)))\n#model.add(LSTM(units=64, activation = 'relu'))\n#model.add(Dense(128, activation='relu', input_dim=input_dim))\n#model.add(Dense(256, activation='relu'))\n#model.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='relu'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = Adam(lr=1e-4),\n              loss='mse',\n              metrics=[rmse])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"history = model.fit_generator(generator=train_generator,\n                              validation_data=val_generator,\n                              epochs = epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyse results\nA short analysis of the train results.","metadata":{}},{"cell_type":"code","source":"loss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'bo', label='loss_train')\nplt.plot(epochs, loss_val, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['rmse']\nacc_val = history.history['val_rmse']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, acc, 'bo', label='accuracy_train')\nplt.plot(epochs, acc_val, 'b', label='accuracy_val')\nplt.title('accuracy')\nplt.xlabel('epochs')\nplt.ylabel('value of accuracy')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Delete train data","metadata":{}},{"cell_type":"code","source":"del train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict test data\n* We following the steps above to prepare the data\n* Build data generator\n* Predict subdate\n* Write data in an array","metadata":{}},{"cell_type":"code","source":"nrows = 1667904\nbatch_size = 1022\nsteps = 25\ny_test = np.empty(())\ntest_weather = pd.read_csv(path_in+'weather_test.csv', parse_dates=['timestamp'])\ncols_with_missing_test_weather = [col for col in test_weather.columns if test_weather[col].isnull().any()]\ntest_weather[cols_with_missing_test_weather] = imp_most.fit_transform(test_weather[cols_with_missing_test_weather])\n\nmean = test_weather[weather_scale].mean(axis=0)\ntest_weather[weather_scale] = test_weather[weather_scale].astype('float32')\ntest_weather[weather_scale] -= test_weather[weather_scale].mean(axis=0)\nstd = test_weather[weather_scale].std(axis=0)\ntest_weather[weather_scale] /= test_weather[weather_scale].std(axis=0)\n\ntest_weather['wind_direction'+'_sin'] = np.sin((2*np.pi*test_weather['wind_direction'])/360)\ntest_weather['wind_direction'+'_cos'] = np.cos((2*np.pi*test_weather['wind_direction'])/360)\ntest_weather = test_weather.drop(['wind_direction'], axis=1)\n\nfor i in range(0, steps):\n    print('work on step ', (i+1))\n    test_data = pd.read_csv(path_in+'test.csv', skiprows=range(1,i*(nrows)+1), nrows=nrows, parse_dates=['timestamp'])\n    test_data['month'] = test_data['timestamp'].dt.month\n    test_data['day'] = test_data['timestamp'].dt.weekday\n    test_data['year'] = test_data['timestamp'].dt.year\n    test_data['hour'] = test_data['timestamp'].dt.hour\n    test_data['weekend'] = np.where((test_data['day'] == 5) | (test_data['day'] == 6), 1, 0)\n    for feature in features_cyc.keys():\n        test_data[feature+'_sin'] = np.sin((2*np.pi*test_data[feature])/features_cyc[feature])\n        test_data[feature+'_cos'] = np.cos((2*np.pi*test_data[feature])/features_cyc[feature])\n    test_data = test_data.drop(features_cyc.keys(), axis=1)\n    test_data = pd.get_dummies(test_data, columns=['meter'])\n    test_data = pd.merge(test_data, building_data, on='building_id', right_index=True)\n    test_data = test_data.sort_values(['timestamp'])\n    test_data = pd.merge_asof(test_data, test_weather, on='timestamp', by='site_id', right_index=True)\n    test_data = test_data.sort_values(['row_id'])\n    for feature in features:\n        if feature not in test_data:\n            #print('   not in:', feature)\n            test_data[feature] = 0\n    test_generator = DataGenerator(test_data, test_data.index, features, batch_size)\n    predict = model.predict_generator(test_generator, verbose=1, workers=1)\n    predict = np.expm1(predict)\n    y_test = np.vstack((y_test, predict))\n    del test_data\n    del test_generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.delete(y_test, 0, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Delete data","metadata":{}},{"cell_type":"code","source":"del test_weather\ndel building_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write output for submission","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'row_id': range(0, len(y_test)),\n                       'meter_reading': y_test.reshape(len(y_test))})\noutput = output[['row_id', 'meter_reading']]\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}